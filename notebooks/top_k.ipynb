{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64578be8-3770-405a-a9e7-c1f8a8180b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/projects/bhuang/.cache/huggingface\"\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63738a82-0579-443e-a9ce-e29e1fa05a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhuang/anaconda3/envs/ocd-10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5b22d-ae53-4e34-8d61-46e40e59bb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "040d375e-e36f-4d11-accd-33572d34828e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59bbe3-afd3-4d9b-8c1b-876795091cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drbenchmark_quaero\n",
    "\n",
    "data_files = {\n",
    "    \"train\": [\n",
    "        \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-medline-train-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "        \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-emea-train-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "    ],\n",
    "    \"valid\": [\n",
    "        \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-medline-validation-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "        \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-emea-validation-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "    ],\n",
    "    # \"test\": [\n",
    "    #     \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-medline-test-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "    #     \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-emea-test-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "    # ],\n",
    "    \"test_quaero_medline\": [\n",
    "        \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-medline-test-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "    ],\n",
    "    \"test_quaero_emea\": [\n",
    "        \"/home/bhuang/icd_10/data/quaero_icd10_by_category_resplitted/drbenchmark_quaero-emea-test-cls-mistral_large_instruct_2407-processed.jsonl\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0046b-4376-41aa-8952-1837bd2f2233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31bbcd8-de01-4484-bf30-56ae37f591ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic\n",
    "\n",
    "data_files = {\n",
    "    \"train\": [\n",
    "        \"/home/bhuang/icd_10/data/synthetic/synthetic-mistral_large_instruct_2407-240909-processed-train-10k.jsonl\",\n",
    "    ],\n",
    "    \"valid\": [\n",
    "        \"/home/bhuang/icd_10/data/synthetic/synthetic-mistral_large_instruct_2407-240909-processed-validation.jsonl\",\n",
    "        \"/home/bhuang/icd_10/data/synthetic_test/synthetic-head-processed-validation.jsonl\",\n",
    "        \"/home/bhuang/icd_10/data/synthetic_test/synthetic-medium-processed-validation.jsonl\",\n",
    "        \"/home/bhuang/icd_10/data/synthetic_test/synthetic-tail-processed-validation.jsonl\",\n",
    "    ],\n",
    "    \"test_synthetic\": [\n",
    "        \"/home/bhuang/icd_10/data/synthetic/synthetic-mistral_large_instruct_2407-240909-processed-test.jsonl\",\n",
    "    ],\n",
    "    \"test_synthetic_head\": [\n",
    "        \"/home/bhuang/icd_10/data/synthetic_test/synthetic-head-processed-test.jsonl\",\n",
    "    ],\n",
    "    \"test_synthetic_medium\": [\n",
    "        \"/home/bhuang/icd_10/data/synthetic_test/synthetic-medium-processed-test.jsonl\",\n",
    "    ],\n",
    "    \"test_synthetic_tail\": [\n",
    "        \"/home/bhuang/icd_10/data/synthetic_test/synthetic-tail-processed-test.jsonl\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac61f68-93f5-4680-8331-e0b4172624bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa0d9ee-5ddf-401a-9b97-f0271655ebe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text', 'has_diso'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['labels', 'text', 'has_diso'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test_synthetic: Dataset({\n",
       "        features: ['labels', 'text', 'has_diso'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_synthetic_head: Dataset({\n",
       "        features: ['labels', 'text', 'has_diso'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_synthetic_medium: Dataset({\n",
       "        features: ['labels', 'text', 'has_diso'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_synthetic_tail: Dataset({\n",
       "        features: ['labels', 'text', 'has_diso'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b851e-312f-495f-895a-ac18615cc1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7f63b7-4382-4be1-b449-4493fbcb4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = dataset[\"train\"][\"text\"], dataset[\"train\"][\"labels\"]\n",
    "# x_valid, y_valid = dataset[\"valid\"][\"text\"], dataset[\"valid\"][\"labels\"]\n",
    "# x_test, y_test = dataset[\"test\"][\"text\"], dataset[\"test\"][\"labels\"]\n",
    "\n",
    "x, y = {}, {}\n",
    "for name, ds in dataset.items():\n",
    "    x[name] = ds[\"text\"]\n",
    "    y[name] = ds[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8095c1-c37d-437e-b9f2-8cd362cbcb87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da77dd-5218-4cba-9e86-3713ce9ae642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f7cfd3a-8bba-4157-95b4-487153fbd99b",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f4accf-b858-4632-93e3-62895c15ec9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# prepare labels\n",
    "\n",
    "# include valid and test in overall classes\n",
    "# y = y_train + y_valid + y_test\n",
    "y_all = sum(y.values(), [])\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y_all)\n",
    "\n",
    "classes = mlb.classes_\n",
    "num_classes = len(classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b129a62-fd3a-4951-a5a2-0dd68f74d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform labels\n",
    "# y_train_encoded = mlb.transform(y_train)\n",
    "# y_valid_encoded = mlb.transform(y_valid)\n",
    "# y_test_encoded = mlb.transform(y_test)\n",
    "\n",
    "y_encoded = {k: mlb.transform(v) for k, v in y.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ece9e1-7058-41e3-8a84-8aa5e00da404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85c26d12-de62-4c5a-bea3-552351a321c0",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b3b80-c74c-4f80-bd34-02989ff0e499",
   "metadata": {},
   "source": [
    "### Evaluate helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d10ec7-8106-4187-aab0-6a3fc43db56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "# fmt: off\n",
    "def evaluate(y, preds, average=\"micro\", verbose=True):\n",
    "    \"\"\"evaluate on all metrics\"\"\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y, preds, average=average, zero_division=1)\n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(y, preds, average=average, labels=classes, zero_division=1)\n",
    "    auc_score = roc_auc_score(y, preds, average=average)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, auc_score: {auc_score:.4f}\")\n",
    "\n",
    "    # print(classification_report(y, preds, zero_division=1, digits=4))\n",
    "\n",
    "    \"\"\"\n",
    "    conf_mat = confusion_matrix(y, preds)\n",
    "    conf_mat_df = pd.DataFrame(conf_mat, index=classes, columns=classes)\n",
    "    # print(conf_mat_df)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(conf_mat_df, annot=True, vmin=0, vmax=conf_mat.max(), fmt='d', cmap=\"YlGnBu\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=45)\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"auc_score\": auc_score}\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80637ab5-b59f-4bff-a856-720bcfaf1e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "950c03c1-2cca-48e9-88aa-629a8af7ee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of labels: 30384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[48, 323, 316, 910, 512]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# \"cheat\" to include valid and test since they have a really poor label coverage\n",
    "# y_flattened = [item for sublist in y_all for item in sublist]\n",
    "y_flattened = [item for sublist in y[\"train\"] for item in sublist]\n",
    "\n",
    "# get label id\n",
    "y_id_flattened = [np.argwhere(mlb.classes_ == item)[0][0] for item in y_flattened]\n",
    "print(\"Total num of labels:\", len(y_id_flattened))\n",
    "\n",
    "# counter of labels in train/valid/test sets\n",
    "counter = Counter(y_id_flattened)\n",
    "\n",
    "\n",
    "def k_most_frequent(k):\n",
    "    return [item for item, _ in counter.most_common(k)]\n",
    "\n",
    "\n",
    "k_most_frequent(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ee9fb-f433-492e-bf8b-8542a3f8db3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2456153-ba82-40cb-80ac-381283304992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(n, k):\n",
    "    preds = np.zeros((n, num_classes), dtype=np.int64)\n",
    "    preds[:, k_most_frequent(k)] = 1\n",
    "    # preds.sum(1)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def predict_evaluate(x, y, k, verbose=True):\n",
    "    \"\"\"predict then evaluate\"\"\"\n",
    "    # predict class labels\n",
    "    preds = predict(len(y), k)\n",
    "    return evaluate(y, preds, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4f4e9-6038-4f81-969f-4e21963aaca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50dc0c9f-4fd2-4a87-a69e-a8418ab20c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.020146</td>\n",
       "      <td>0.162276</td>\n",
       "      <td>0.035842</td>\n",
       "      <td>0.568495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.247441</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>0.603150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.129468</td>\n",
       "      <td>0.035101</td>\n",
       "      <td>0.554727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.019558</td>\n",
       "      <td>0.170666</td>\n",
       "      <td>0.035094</td>\n",
       "      <td>0.571628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.018762</td>\n",
       "      <td>0.270767</td>\n",
       "      <td>0.035092</td>\n",
       "      <td>0.612699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  precision    recall        f1  auc_score\n",
       "24  24   0.020146  0.162276  0.035842   0.568495\n",
       "39  39   0.018904  0.247441  0.035124   0.603150\n",
       "19  19   0.020303  0.129468  0.035101   0.554727\n",
       "26  26   0.019558  0.170666  0.035094   0.571628\n",
       "43  43   0.018762  0.270767  0.035092   0.612699"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search k on validation set\n",
    "\n",
    "perf_by_k = []\n",
    "for k in range(0, 51):\n",
    "    # r = predict_evaluate(x_valid, y_valid_encoded, k=k, verbose=False)\n",
    "    r = predict_evaluate(x[\"valid\"], y_encoded[\"valid\"], k=k, verbose=False)\n",
    "    perf_by_k.append({\"k\": k, **r})\n",
    "\n",
    "df_perf_by_k = pd.DataFrame(perf_by_k)\n",
    "# sort by f1\n",
    "df_perf_by_k = df_perf_by_k.sort_values(\"f1\", ascending=False)\n",
    "df_perf_by_k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb3dff-eacf-44f3-bfa6-e94e9d31efdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d66069-844f-437a-b78b-9359ddf08165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best k: 24\n",
      "perf on test set test_synthetic --> precision: 0.0225, recall: 0.1763, f1: 0.0400, auc_score: 0.5756\n",
      "perf on test set test_synthetic_head --> precision: 0.0519, recall: 0.4090, f1: 0.0921, auc_score: 0.6923\n",
      "perf on test set test_synthetic_medium --> precision: 0.0000, recall: 0.0000, f1: 0.0000, auc_score: 0.4871\n",
      "perf on test set test_synthetic_tail --> precision: 0.0056, recall: 0.0444, f1: 0.0100, auc_score: 0.5094\n"
     ]
    }
   ],
   "source": [
    "# eval on test set using determined k\n",
    "best_k = int(df_perf_by_k.iloc[0][\"k\"])\n",
    "print(f\"best k: {best_k}\")\n",
    "\n",
    "# print(\"perf on test set --> \", end=\"\")\n",
    "# predict_evaluate(x_test, y_test_encoded, k=best_k)\n",
    "\n",
    "result = {}\n",
    "for split in x:\n",
    "    if split.startswith(\"test\"):\n",
    "        print(f\"perf on test set {split} --> \", end=\"\")\n",
    "        r = predict_evaluate(x[split], y_encoded[split], k=best_k)\n",
    "        result.update({f\"{split}_{k}\": v for k, v in r.items()})\n",
    "\n",
    "# save result\n",
    "df_result = pd.DataFrame([result])\n",
    "# df_result.to_json(\"tmp_result/tmp.json\", orient=\"records\", lines=True, force_ascii=False)\n",
    "df_result.to_csv(\"tmp_result/tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b03fd-2c22-4e72-8a9d-764bd456e560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocd-10",
   "language": "python",
   "name": "ocd-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
